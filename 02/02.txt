02.txt

线性回归
代码:line.py
保存和载入模型
import pickle 
pickle.dump(内存对象,文件对象)
pickle.load(文件对象)->内存对象
代码:dump.py,load.py

四.岭回归
loss' = loss - C x 正则项(x,y)
C 正则强度,越大越影响正则项
岭回归本质就是在线性回归的基础上增加了正则项,有意去破坏模型对训练数据集的拟合效果
,让实际损失值大一点,客观上就降低了少数异常样本对模型的牵制作用,使得模型对大多数正常样本表现出更好的拟合效果.
岭回归器=lm.Ridge(alpha = C)
代码:rdg.py

五.多项式回归
y' = w0 + w1x w2x^2 + w3x^3 +...+ wnx^n
x->多项式特征扩展器-x1...xn->+线性回归器->w0...wn
		\-------------------------/
					|
				管线(流水线)
代码:poly.py

六.决策树
回归问题:输出标签分布于无限连续域.
分类问题:输出标签分布于有限离散域.
核心思想:相似的因会导致相似的果.

驾龄 均速 -> 性别
1     60      m
2     40      w
3     80      m
1     40      w
2     80      m
1     40      w
1     40      m
3     60      w
2     60      m
3     80      w
----------------
1 60 m   2 40 w    3 80  m
1 40 w   2  80 m   3 60  w
1 40 w   2  60 m   3 80  w
1 40 m
------------------------------------------------------------------
1 60 m   2 40 w     3 80  m
                                3 80  w
1 40 w   2  80 m
1 40 w                     3 60  w
1 40 m   2  60 m
------------------------------
1 40 ? -> w


1.原理
一次选择原始输入样本矩阵中的每一个特征,根据其取值的不同划分若干子集,使该子集中相应特征取相同的值,借以提高样本集合的信息量,即减少信息熵,当全部特征都被用尽时,即完成整颗决策树的构建.对于未知输出的待预测样本,根据特征的相似性,将其归属到相似程度最高的叶子集中,通过投票(分类)或者平均(回归)获得其与之对应的预测结果.
2.优化
根据数据集划分前后信息熵的减少量,优先选择部分可使熵减最大的特征进行决策树的构建,提前结束划分过程.牺牲部分次要特征,换取简化模型的性能提升.
3.集合算法
按照某种规则,利用原始的输入样本牛作为基础,构建多棵体现不同样本子集或特征子集的决策树模型,以投票或平均的方式综合多个模型的预测结果,形成相对一般(泛化)的最终结果.
1)自助聚合
在全部n个样本中,以有放回抽样的方式,选取m个样本,构建一棵决策树模型,重复以上过程b次,得到b棵决策树模型.
2)随机森林
在自助聚合的基础上,构建单棵决策树的时候不但随机选取m个样本,而且随机选取p个特征,利用对样本和特征的双重泛化,进一步削弱特殊样本和特征对模型预测结果的影响.
3)正向激励
初始化时,给训练集中的每个样本分配相等的权重,构建第一个决策树模型,然后用该模型对训练集进行预测,针对预测错误的样本提升其权重,再构建第二棵决策树模型,以此类推,最终得到b棵样本权重各不相同的决策树模型.利用权重的差别,均化部分特殊样本对预测结果的影响.
import sklearn.tree as st 
st.DecisionTreeRegressor()    \单棵决策树
st.DecisionTreeClassifier()   /
import sklearn.ensemble as se 
se.AdaBoostRegressor()       \正向激励
se.AdaBoostClassifier()      /
se.RandomForestRegressor()   \随机森林
se.RandomForestClassifier()  /
代码:house.py
4.特征重要性
所有基于决策树的学习模型,在确定划分子集所用特征时,都会遵循信息熵减少量最大化原则,由此可以得知不同特征的重要性指标:决策树模型对象.feature_importances_
超参数:事先认为给定参数,如正则强度,评估器数等等.
模型参数:决定预测算法的参数,如线性回归中的斜率和截距
学习参数:在模型学习训练数据的过程中,额外提供的一些中间参数,特征重要性等.
特征重要性一方面与模型使用的算法有关,另一方面还与训练数据的粒度有关.
代码:fi.py
共享单车需求数据分析
代码:bike.py

七.简单分类
x1 x2 y
3   1   0
2   5   1
1   8   1
6   4   0
5   2   0
3   5   1
4   7   1
4  -1  0
--------
8   2   ?->0
4   9   ?->1
x1>x2->y=0
x1<x2->y=1
代码：simple.py
------------------------